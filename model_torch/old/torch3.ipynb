{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#https://colab.research.google.com/drive/1N3LvAO0AXV4kBPbTMX866OwJM9YS6Ji2?usp=sharing#scrollTo=fl5W1gg5Jhzz\n",
    "\n",
    "df_players = pd.read_csv(\"./../data_scrapped/atp_players.csv\")\n",
    "df_matchs = pd.read_csv(\"./../data_formatted/training_dgl_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On construit le graph\n",
    "\n",
    "# lecture des données attendues\n",
    "# tensor1[1] -> tensor2[1] \n",
    "# le player index en tensor1 a joué contre le player au meme index en tensor 2\n",
    "# y stocke le résultat du match\n",
    "\n",
    "\n",
    "tensor1=[]\n",
    "tensor2=[]\n",
    "\n",
    "for index,row in df_players.iterrows():\n",
    "    winmatchs = df_matchs[df_matchs.player1_id == row.player_id]\n",
    "    #print(len(winmatchs))\n",
    "    if len(winmatchs) > 0:\n",
    "        #print(row.player_id)\n",
    "        for index2, row2 in winmatchs.iterrows():\n",
    "                tensor1.append(index)\n",
    "                tensor2.append(df_players.loc[df_players.player_id == row2.player2_id].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "labels = numpy.ones(len(tensor1))\n",
    "\n",
    "indexes = random.sample(range(0, len(tensor1)-1), ((len(tensor1)-1)//2))\n",
    "\n",
    "for index in indexes:\n",
    "    temp = tensor1[index]\n",
    "    tensor1[index] = tensor2[index]\n",
    "    tensor2[index] = temp\n",
    "    labels[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chbou\\AppData\\Local\\Temp\\ipykernel_15768\\3264489422.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3446, 3], edge_index=[2, 51981], edge_label=[51981])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create the heterogeneous graph data object:\n",
    "#data = Data()\n",
    "\n",
    "# Add the user nodes:\n",
    "x = torch.tensor(list(df_players[[\"birth_year\",\"weight_kg\",\"height_cm\"]].values),dtype=torch.float)  # [num_users, num_features_users]\n",
    "x = torch.nan_to_num(x, nan=0.0)\n",
    "#x = torch.masked_select(x, ~torch.isnan(x))\n",
    "#x = torch.ones(df_players.shape[0])\n",
    "\n",
    "edge_index = torch.stack([torch.tensor(tensor1), torch.tensor(tensor2)], dim=0)\n",
    "#edge_attr = torch.Tensor(list(df_matchs[[\"player1_atprank\",\"player1_oddsB365\",\"player2_atprank\",\"player2_oddsB365\"]].values))\n",
    "#y = torch.Tensor(list(df_matchs[[\"winner_player1\"]].values))\n",
    "\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index,edge_label=labels) \n",
    "#data = T.ToUndirected()(data)\n",
    "# Add the movie nodes:\n",
    "#data['movie'].x = movie_features  # [num_movies, num_features_movies]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.,  ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[3446, 3], edge_index=[2, 44184], edge_label=[44184], edge_label_index=[2, 44184]),\n",
       " Data(x=[3446, 3], edge_index=[2, 44184], edge_label=[2599], edge_label_index=[2, 2599]),\n",
       " Data(x=[3446, 3], edge_index=[2, 46783], edge_label=[5198], edge_label_index=[2, 5198]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.05,\n",
    "    num_test=0.1,\n",
    "    is_undirected=False,\n",
    "    add_negative_train_samples=False,\n",
    "    neg_sampling_ratio=0\n",
    ")(data)\n",
    "train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 0, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.edge_label.unique())\n",
    "print(test_data.edge_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2083,  242, 2642,  ..., 2342, 2061, 1530],\n",
       "        [2624, 3252, 1093,  ..., 1608, 2321,  979]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2083,  242, 2642,  ..., 2342, 2061, 1530],\n",
       "        [2624, 3252, 1093,  ..., 1608, 2321,  979]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv,MLP\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.mlp = MLP(in_channels=6, hidden_channels=hidden_channels, out_channels=1, num_layers=3)\n",
    "        #self.fc = nn.MLP(6, 1) # Adjust output size for edge direction prediction\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return self.mlp(torch.cat((x[edge_label_index[0]], x[edge_label_index[1]]), dim=1))\n",
    "    #(z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    #def decode_all(self, z):\n",
    "    #    prob_adj = z @ z.t()\n",
    "    #    return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(3, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1990.,   70.,  180.],\n",
       "        [1986.,   79.,  183.],\n",
       "        [1982.,   73.,  183.],\n",
       "        ...,\n",
       "        [1978.,   80.,  180.],\n",
       "        [1987.,   77.,  188.],\n",
       "        [1981.,   80.,  183.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[train_data.edge_label_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7017, Val: 0.5194, Test: 0.5459\n",
      "Epoch: 002, Loss: 0.7221, Val: 0.5165, Test: 0.5420\n",
      "Epoch: 003, Loss: 0.6992, Val: 0.5202, Test: 0.5434\n",
      "Epoch: 004, Loss: 0.6923, Val: 0.5235, Test: 0.5446\n",
      "Epoch: 005, Loss: 0.6910, Val: 0.5258, Test: 0.5442\n",
      "Epoch: 006, Loss: 0.6907, Val: 0.5314, Test: 0.5472\n",
      "Epoch: 007, Loss: 0.6906, Val: 0.5336, Test: 0.5486\n",
      "Epoch: 008, Loss: 0.6903, Val: 0.5337, Test: 0.5483\n",
      "Epoch: 009, Loss: 0.6908, Val: 0.5332, Test: 0.5484\n",
      "Epoch: 010, Loss: 0.6907, Val: 0.5336, Test: 0.5490\n",
      "Epoch: 011, Loss: 0.6897, Val: 0.5321, Test: 0.5488\n",
      "Epoch: 012, Loss: 0.6897, Val: 0.5309, Test: 0.5482\n",
      "Epoch: 013, Loss: 0.6895, Val: 0.5297, Test: 0.5473\n",
      "Epoch: 014, Loss: 0.6892, Val: 0.5283, Test: 0.5463\n",
      "Epoch: 015, Loss: 0.6888, Val: 0.5273, Test: 0.5456\n",
      "Epoch: 016, Loss: 0.6885, Val: 0.5267, Test: 0.5450\n",
      "Epoch: 017, Loss: 0.6887, Val: 0.5266, Test: 0.5451\n",
      "Epoch: 018, Loss: 0.6883, Val: 0.5267, Test: 0.5454\n",
      "Epoch: 019, Loss: 0.6882, Val: 0.5266, Test: 0.5454\n",
      "Epoch: 020, Loss: 0.6883, Val: 0.5252, Test: 0.5447\n",
      "Epoch: 021, Loss: 0.6882, Val: 0.5251, Test: 0.5446\n",
      "Epoch: 022, Loss: 0.6880, Val: 0.5253, Test: 0.5447\n",
      "Epoch: 023, Loss: 0.6878, Val: 0.5254, Test: 0.5448\n",
      "Epoch: 024, Loss: 0.6876, Val: 0.5255, Test: 0.5451\n",
      "Epoch: 025, Loss: 0.6877, Val: 0.5256, Test: 0.5455\n",
      "Epoch: 026, Loss: 0.6874, Val: 0.5259, Test: 0.5459\n",
      "Epoch: 027, Loss: 0.6873, Val: 0.5258, Test: 0.5464\n",
      "Epoch: 028, Loss: 0.6872, Val: 0.5256, Test: 0.5466\n",
      "Epoch: 029, Loss: 0.6871, Val: 0.5249, Test: 0.5465\n",
      "Epoch: 030, Loss: 0.6869, Val: 0.5250, Test: 0.5473\n",
      "Epoch: 031, Loss: 0.6867, Val: 0.5248, Test: 0.5472\n",
      "Epoch: 032, Loss: 0.6868, Val: 0.5250, Test: 0.5475\n",
      "Epoch: 033, Loss: 0.6865, Val: 0.5250, Test: 0.5478\n",
      "Epoch: 034, Loss: 0.6865, Val: 0.5252, Test: 0.5478\n",
      "Epoch: 035, Loss: 0.6865, Val: 0.5256, Test: 0.5479\n",
      "Epoch: 036, Loss: 0.6863, Val: 0.5257, Test: 0.5478\n",
      "Epoch: 037, Loss: 0.6862, Val: 0.5254, Test: 0.5475\n",
      "Epoch: 038, Loss: 0.6862, Val: 0.5255, Test: 0.5479\n",
      "Epoch: 039, Loss: 0.6860, Val: 0.5256, Test: 0.5480\n",
      "Epoch: 040, Loss: 0.6860, Val: 0.5257, Test: 0.5483\n",
      "Epoch: 041, Loss: 0.6859, Val: 0.5254, Test: 0.5481\n",
      "Epoch: 042, Loss: 0.6858, Val: 0.5243, Test: 0.5478\n",
      "Epoch: 043, Loss: 0.6857, Val: 0.5232, Test: 0.5479\n",
      "Epoch: 044, Loss: 0.6856, Val: 0.5226, Test: 0.5481\n",
      "Epoch: 045, Loss: 0.6855, Val: 0.5224, Test: 0.5483\n",
      "Epoch: 046, Loss: 0.6854, Val: 0.5225, Test: 0.5483\n",
      "Epoch: 047, Loss: 0.6853, Val: 0.5228, Test: 0.5486\n",
      "Epoch: 048, Loss: 0.6852, Val: 0.5229, Test: 0.5491\n",
      "Epoch: 049, Loss: 0.6852, Val: 0.5228, Test: 0.5492\n",
      "Epoch: 050, Loss: 0.6851, Val: 0.5228, Test: 0.5486\n",
      "Epoch: 051, Loss: 0.6849, Val: 0.5233, Test: 0.5483\n",
      "Epoch: 052, Loss: 0.6848, Val: 0.5240, Test: 0.5487\n",
      "Epoch: 053, Loss: 0.6847, Val: 0.5249, Test: 0.5492\n",
      "Epoch: 054, Loss: 0.6846, Val: 0.5222, Test: 0.5473\n",
      "Epoch: 055, Loss: 0.6845, Val: 0.5204, Test: 0.5416\n",
      "Epoch: 056, Loss: 0.6844, Val: 0.5223, Test: 0.5425\n",
      "Epoch: 057, Loss: 0.6843, Val: 0.5210, Test: 0.5460\n",
      "Epoch: 058, Loss: 0.6841, Val: 0.5253, Test: 0.5491\n",
      "Epoch: 059, Loss: 0.6841, Val: 0.5270, Test: 0.5505\n",
      "Epoch: 060, Loss: 0.6840, Val: 0.5271, Test: 0.5521\n",
      "Epoch: 061, Loss: 0.6838, Val: 0.5272, Test: 0.5499\n",
      "Epoch: 062, Loss: 0.6837, Val: 0.5258, Test: 0.5468\n",
      "Epoch: 063, Loss: 0.6836, Val: 0.5253, Test: 0.5464\n",
      "Epoch: 064, Loss: 0.6835, Val: 0.5235, Test: 0.5442\n",
      "Epoch: 065, Loss: 0.6833, Val: 0.5223, Test: 0.5426\n",
      "Epoch: 066, Loss: 0.6832, Val: 0.5212, Test: 0.5445\n",
      "Epoch: 067, Loss: 0.6829, Val: 0.5215, Test: 0.5450\n",
      "Epoch: 068, Loss: 0.6828, Val: 0.5159, Test: 0.5396\n",
      "Epoch: 069, Loss: 0.6825, Val: 0.5074, Test: 0.5267\n",
      "Epoch: 070, Loss: 0.6824, Val: 0.5094, Test: 0.5281\n",
      "Epoch: 071, Loss: 0.6823, Val: 0.5031, Test: 0.5141\n",
      "Epoch: 072, Loss: 0.6824, Val: 0.4991, Test: 0.5100\n",
      "Epoch: 073, Loss: 0.6822, Val: 0.5050, Test: 0.5116\n",
      "Epoch: 074, Loss: 0.6823, Val: 0.5023, Test: 0.5069\n",
      "Epoch: 075, Loss: 0.6818, Val: 0.4972, Test: 0.5026\n",
      "Epoch: 076, Loss: 0.6822, Val: 0.5053, Test: 0.5109\n",
      "Epoch: 077, Loss: 0.6813, Val: 0.5136, Test: 0.5250\n",
      "Epoch: 078, Loss: 0.6813, Val: 0.5131, Test: 0.5281\n",
      "Epoch: 079, Loss: 0.6811, Val: 0.5138, Test: 0.5343\n",
      "Epoch: 080, Loss: 0.6810, Val: 0.4835, Test: 0.4828\n",
      "Epoch: 081, Loss: 0.6827, Val: 0.5034, Test: 0.5142\n",
      "Epoch: 082, Loss: 0.6812, Val: 0.5156, Test: 0.5358\n",
      "Epoch: 083, Loss: 0.6814, Val: 0.5166, Test: 0.5363\n",
      "Epoch: 084, Loss: 0.6818, Val: 0.4845, Test: 0.4866\n",
      "Epoch: 085, Loss: 0.6819, Val: 0.4771, Test: 0.4763\n",
      "Epoch: 086, Loss: 0.6816, Val: 0.5177, Test: 0.5355\n",
      "Epoch: 087, Loss: 0.6812, Val: 0.5163, Test: 0.5388\n",
      "Epoch: 088, Loss: 0.6806, Val: 0.5142, Test: 0.5381\n",
      "Epoch: 089, Loss: 0.6799, Val: 0.5111, Test: 0.5350\n",
      "Epoch: 090, Loss: 0.6798, Val: 0.5130, Test: 0.5369\n",
      "Epoch: 091, Loss: 0.6830, Val: 0.5119, Test: 0.5363\n",
      "Epoch: 092, Loss: 0.6810, Val: 0.5042, Test: 0.5321\n",
      "Epoch: 093, Loss: 0.6810, Val: 0.5204, Test: 0.5315\n",
      "Epoch: 094, Loss: 0.6823, Val: 0.5237, Test: 0.5378\n",
      "Epoch: 095, Loss: 0.6809, Val: 0.5240, Test: 0.5322\n",
      "Epoch: 096, Loss: 0.6815, Val: 0.5245, Test: 0.5352\n",
      "Epoch: 097, Loss: 0.6811, Val: 0.5228, Test: 0.5292\n",
      "Epoch: 098, Loss: 0.6806, Val: 0.5276, Test: 0.5529\n",
      "Epoch: 099, Loss: 0.6810, Val: 0.5296, Test: 0.5489\n",
      "Epoch: 100, Loss: 0.6808, Val: 0.5244, Test: 0.5276\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    out = model.decode(z, train_data.edge_label_index)#.view(-1)\n",
    "    loss = criterion(out.squeeze(), train_data.edge_label.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index)#.view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.float(), out.squeeze())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\CODE\\ParisTennis\\model_torch\\torch1bis.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/CODE/ParisTennis/model_torch/torch1bis.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m out\u001b[39m.\u001b[39mdtypes()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out.dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16474\n",
      "16474\n",
      "51981\n",
      "0.6338469825513168\n"
     ]
    }
   ],
   "source": [
    "print(len(df_matchs[(df_matchs.player1_oddsB365 < df_matchs.player2_oddsB365) & (df_matchs.winner_player1 == 1)]))\n",
    "print(len(df_matchs[(df_matchs.player1_oddsB365 > df_matchs.player2_oddsB365) & (df_matchs.winner_player1 == 0)]))\n",
    "print(len(df_matchs))\n",
    "print(16474 * 2 / 51981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1035,  969],\n",
      "        [ 969, 1035]])\n",
      "tensor([[0.0969],\n",
      "        [0.2451]])\n"
     ]
    }
   ],
   "source": [
    "# Specify the edge you want to predict (e.g., edge from node 0 to node 1)\n",
    "node_0 = 1035\n",
    "node_1 = 969\n",
    "\n",
    "# Predict the direction of the edge\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    edge_label_index =  torch.tensor([[node_0, node_1], [node_1, node_0]], dtype=torch.long).t()\n",
    "    print(edge_label_index)\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    prediction = model.decode(z, edge_label_index)#.view(-1).sigmoid()\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
